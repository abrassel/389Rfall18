Writeup 1 - Ethics
======

Name: Alexander Brassel
Section: 0201

I pledge on my honor that I have not given or received any unauthorized assistance on this assignment or examniation.

Digital acknowledgement: _Alexander Brassel_

## Assignment Writeup

### Part 1 (25 pts)

This was done via the [ELMS assignment](https://myelms.umd.edu/courses/1251976/assignments/4726433).

### Part 2 (75 pts)

This is a very difficult question, largely due to how complex the context is. I believe that one of the moral duties of a software developer is to look out for the well-being of the users of my product. However, there is also a conflicting moral obligation to provide the best possible service to the company that has hired me. Many companies seem to have an official avenue for publishing vulnerabilities and weaknesses in their code and services. Additionally, there may be extenuating circumstances that I am not aware of. Therefore, I believe that an appropriate first course of action is to contact my surperior and inform them of the error I have found. This gives them the opportunity to handle the vulnerability through official channels, and potentially work on some company PR.  However, if they then fail to responsibly handle the vulnerabilities and report them to the world, the developer (in this example me) then has a hypothetical duty to contact both watchdog agencies and the world.

There are, of course, counter arguments to doing this. I personally believe the most compelling one is that you may just not know better - you are a junior developer and there are many factors at play in large companies that you are blissfully unaware of. How could you possibly know what is right for the company and the world? That's why I included the first step of contacting your surperior. This gives the company a chance to make reasonable choices before choosing the nuclear option.

Regardless of your choices, I would argue that unless you have been forcibly compelled to remain silent on the issue, you bare fault for any potential accidents and breaches of security. A good analogous example is the Challenger. Many engineers reported to management that the O-Rings were not properly sealed, but admins at NASA decided to push through with the launch anyway. Had engineers not reported the problem at all, they would have been grossly to blame for the failure. As it was, its slightly more nebulous, but they failed to report to the public about a public health risk that was being blatantly ignored. They had the power, and thus, the responsibility.
